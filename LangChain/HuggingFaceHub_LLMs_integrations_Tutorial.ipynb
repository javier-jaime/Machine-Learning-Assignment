{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP6pmRleMx8EMNWe7ticsmn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/javier-jaime/Tool-Crib/blob/master/LangChain/HuggingFaceHub_LLMs_integrations_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LangChain & HuggingFaceHub LLMs integrations Tutorial\n",
        "\n",
        "In this tutorial we will use LangChain Open Source Orchestration Framework to integrate Large Language Models from Hugging Face Hub.\n",
        "\n",
        "This tutorial is based on code from: Generative AI with LangChain: Build large language model (LLM) apps with Python, ChatGPT and other LLMs by Ben Auffarth (2023) https://github.com/benman1/generative_ai_with_langchain"
      ],
      "metadata": {
        "id": "EB1Mvtzsd9m8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Notebook Preparation"
      ],
      "metadata": {
        "id": "d4rYayyyhv9M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we will need to install the langchain library, and import all the necessary requirements"
      ],
      "metadata": {
        "id": "zCvLfxbJi1EO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install langchain library the first time, comment/uncomment as required\n",
        "!pip install langchain\n",
        "from langchain_community.llms import HuggingFaceHub\n",
        "from langchain import PromptTemplate, LLMChain\n",
        "from transformers import pipeline\n",
        "\n",
        "# Import operating system and userdata interface from Google Colab\n",
        "import os\n",
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "4-Vb2-HXFwlj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To use Hugging Face as a provider for your models, you need to create an account and get an API Key from: https://huggingface.co/settings/profile"
      ],
      "metadata": {
        "id": "-jWYpfD7kSFm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "XYwmoAkKD-mM"
      },
      "outputs": [],
      "source": [
        "# Set an API key directly to the Python environment, comment/uncomment as required\n",
        "\n",
        "# os.environ['HUGGINGFACEHUB_API_TOKEN'] = '<your API key token>'\n",
        "\n",
        "# Or store the environment variable as a Secret in Colab, and\n",
        "\n",
        "# Access your Secret Key and declare it as an environment variable\n",
        "\n",
        "os.environ['HUGGINGFACEHUB_API_TOKEN'] = userdata.get('HUGGINGFACEHUB_API_TOKEN')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hugging Face Simple Example\n",
        "\n",
        "In this case we are using a simple open source model developed by Google to test a simple prompt,\n",
        "\n",
        "with a temperature of 0.5 (half BS) and a maximum lenght of 64."
      ],
      "metadata": {
        "id": "MFhrkz57mtcT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model\n",
        "llm = HuggingFaceHub(\n",
        "model_kwargs={\"temperature\": 0.5, \"max_length\": 64},\n",
        "repo_id=\"google/flan-t5-xxl\"\n",
        ")"
      ],
      "metadata": {
        "id": "rz5ShjhjGQzI"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pass the prompt\n",
        "prompt = input('Enter your question: ')\n",
        "completion = llm(prompt)\n",
        "print(completion)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cpEOZhkSHO2G",
        "outputId": "a1c38314-d424-4e07-c9b8-655a8106cc2c"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your question: Where is Calgary?\n",
            "canada\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can run a simple model by chaining a prompt template and a LLM"
      ],
      "metadata": {
        "id": "ePKWwp7lrOyq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"Question: {question}\n",
        "Answer: Let's think step by step.\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(template=template, input_variables=['question'])\n",
        "\n",
        "llm_chain = LLMChain(prompt=prompt, llm=llm, verbose=True)\n",
        "\n",
        "test_question = \" Who was the mayor in the year of Calgary Olympics games\"\n",
        "\n",
        "llm_chain.run(test_question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "deDpf7dvpgna",
        "outputId": "0a2771ba-74f8-484f-cc54-dbda97d364a4"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mQuestion:  Who was the mayor in the year of Calgary Olympics games\n",
            "Answer: Let's think step by step.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'In 1988, the Calgary Olympics games were held. The mayor in 1988 was Ken Melamed. The answer: Ken Melamed.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used verbose=True to see the reasoning, but we didn't ge the right result, Ken Melamed was the Major of Vancouver in 1988, let's try again."
      ],
      "metadata": {
        "id": "-GfcfF2VvkqC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \" Who was the Premier of Alberta in the year of Calgary Olympics games\"\n",
        "llm_chain.run(question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "M52MB7EaubU1",
        "outputId": "3b0dfc43-6919-441e-e6c0-777a960f4dce"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mQuestion:  Who was the Premier of Alberta in the year of Calgary Olympics games\n",
            "Answer: Let's think step by step.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Premier of Alberta is the leader of the provincial government of Alberta. The year of Calgary Olympics games was 1988. Ed Stelmach was the Premier of Alberta in 1988. The answer: Ed Stelmach.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wrong again, but close enough for a small model.\n"
      ],
      "metadata": {
        "id": "Pg7jwuRwx_a4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building an Customer Service app\n"
      ],
      "metadata": {
        "id": "AsRAlqi8Hy3E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generative AI can assist customer service agents in several ways:\n",
        "\n",
        "**Sentiment classification:** This helps identify customer emotions and allows agents to personalize their responses.\n",
        "\n",
        "**Summarization:** This enables agents to understand the key points of lengthy customer messages and save time.\n",
        "\n",
        "**Intent classification:** Similar to summarization, this helps predict the customerâ€™s purpose and allows for faster problem-solving.\n",
        "\n",
        "**Answer suggestions:** This provides agents with suggested responses to common inquiries, ensuring that accurate and consistent messaging is provided."
      ],
      "metadata": {
        "id": "vtyG9QmhH_i2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can list the 5 most downloaded models on Hugging Face Hub for each way\n",
        "with Hugging Face API:"
      ],
      "metadata": {
        "id": "hLHO5z3eLVVe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import list_models\n",
        "def list_most_popular(task: str):\n",
        "  print('\\033[1m' + 'Model Id, Model Downloads')\n",
        "  print('---------------------------------')\n",
        "  print('\\033[0m')\n",
        "  for rank, model in enumerate(\n",
        "    list_models(filter=task, sort=\"downloads\", direction=-1)):\n",
        "    if rank == 5:\n",
        "      break\n",
        "    print(f\"{model.id}, {model.downloads}\\n\")"
      ],
      "metadata": {
        "id": "QXZsVKldIDeb"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For text classification\n",
        "list_most_popular(\"text-classification\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "grPDOzLEMOSa",
        "outputId": "8296773b-02c7-4b49-904f-5edddd9dfeb1"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mModel Id, Model Downloads\n",
            "---------------------------------\n",
            "\u001b[0m\n",
            "cardiffnlp/twitter-roberta-base-sentiment-latest, 48893886\n",
            "\n",
            "mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis, 31289930\n",
            "\n",
            "distilbert/distilbert-base-uncased-finetuned-sst-2-english, 13322853\n",
            "\n",
            "lxyuan/distilbert-base-multilingual-cased-sentiments-student, 11632332\n",
            "\n",
            "cardiffnlp/twitter-roberta-base-irony, 11075353\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For intent classification\n",
        "list_most_popular('intent-classification')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJxVKZ2QMZ9D",
        "outputId": "1f3c7cbf-efb5-4dd6-d0f4-1b5d53bad03d"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mModel Id, Model Downloads\n",
            "---------------------------------\n",
            "\u001b[0m\n",
            "qanastek/XLMRoberta-Alexa-Intents-Classification, 2603\n",
            "\n",
            "bespin-global/klue-roberta-small-3i4k-intent-classification, 396\n",
            "\n",
            "cartesinus/xlm-r-base-amazon-massive-intent, 231\n",
            "\n",
            "lxyuan/banking-intent-distilbert-classifier, 78\n",
            "\n",
            "cartesinus/mdeberta-v3-base_amazon-massive_intent, 47\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For summarization\n",
        "list_most_popular('summarization')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZMTt0g5oJchm",
        "outputId": "9325d6bd-1515-483e-eef0-a9ae535ba598"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mModel Id, Model Downloads\n",
            "---------------------------------\n",
            "\u001b[0m\n",
            "google-t5/t5-small, 3649108\n",
            "\n",
            "google-t5/t5-base, 2770111\n",
            "\n",
            "facebook/bart-large-cnn, 2403645\n",
            "\n",
            "philschmid/bart-large-cnn-samsum, 700812\n",
            "\n",
            "sshleifer/distilbart-cnn-12-6, 648192\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For question answering\n",
        "list_most_popular('question-answering')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_EktDYvMh3U",
        "outputId": "2a5367c8-a659-4e0f-e03e-e202da469244"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mModel Id, Model Downloads\n",
            "---------------------------------\n",
            "\u001b[0m\n",
            "deepset/roberta-base-squad2, 1141775\n",
            "\n",
            "Intel/dynamic_tinybert, 460278\n",
            "\n",
            "distilbert/distilbert-base-cased-distilled-squad, 389640\n",
            "\n",
            "FabianWillner/distilbert-base-uncased-finetuned-squad, 374139\n",
            "\n",
            "timpal0l/mdeberta-v3-base-squad2, 284454\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A GPT-3.5 Generated e-mail shortened example"
      ],
      "metadata": {
        "id": "uLiYgNzCPI8Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "customer_email = \"\"\"\n",
        "I am writing to pour my heart out about the recent unfortunate experience\n",
        "I had with one of your coffee machines that arrived broken. I anxiously\n",
        "unwrapped the box containing my highly anticipated coffee machine.\n",
        "However, what I discovered within broke not only my spirit but also any\n",
        "semblance of confidence I had placed in your brand.\n",
        "Its once elegant exterior was marred by the scars of travel, resembling a\n",
        "war-torn soldier who had fought valiantly on the fields of some espresso\n",
        "battlefield. This heartbreaking display of negligence shattered my dreams\n",
        "of indulging in daily coffee perfection, leaving me emotionally distraught\n",
        "and inconsolable\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "N_YO2_O_OutV"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The sentiment model *twitter-roberta-base-sentiment* was trained on tweets, it is the most used but not the most adequate for this use case."
      ],
      "metadata": {
        "id": "hFXDRkC5RUEw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_model = pipeline(\n",
        "task=\"sentiment-analysis\",\n",
        "model=\"cardiffnlp/twitter-roberta-base-sentiment\"\n",
        ")"
      ],
      "metadata": {
        "id": "OV5UCOtoO1Ku"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the sentiment analysis, we got a rating and a numeric score that expresses confidence in the label.\n",
        "\n",
        "The labels are:\n",
        "**0** negative\n",
        "**1** neutral\n",
        "**2** positive"
      ],
      "metadata": {
        "id": "gjcodTrNQrxR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(sentiment_model(customer_email))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dsu8gCatSF9l",
        "outputId": "32feb86e-dbd6-4054-bbe0-d34f7aa4b1c1"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'label': 'LABEL_0', 'score': 0.7691406607627869}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(sentiment_model(\"I am elated, I am so happy, this is the best thing that ever happened to me!\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZKnh-zTSm8g",
        "outputId": "e9c25a0b-1b2f-4bf6-e895-d53820919e05"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'label': 'LABEL_2', 'score': 0.9926880598068237}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(sentiment_model(\"I don't care. I guess it's ok, or not, I couldn't care one way or the other\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UIFv3PpVSp9u",
        "outputId": "7d00def5-1916-4408-be71-0502e7956490"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'label': 'LABEL_1', 'score': 0.5958544611930847}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(sentiment_model(\"I am so angry and sad, I want to kill myself!\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-odq2-9SgIi",
        "outputId": "b584d820-82b6-4dca-d4c9-7d3c3729119a"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'label': 'LABEL_0', 'score': 0.9788626432418823}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For Summarization we can execute the *facebook/bart-large-cnn* remotely from the HuggingFaceHub server, we will need the API Token."
      ],
      "metadata": {
        "id": "gpASEwTOTBQK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summarizer = HuggingFaceHub(\n",
        "repo_id=\"facebook/bart-large-cnn\",\n",
        "model_kwargs={\"temperature\":0, \"max_length\":180}\n",
        ")\n",
        "def summarize(llm, text) -> str:\n",
        "  return llm(f\"Summarize this: {text}!\")\n",
        "summarize(summarizer, customer_email)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "7esmMN6ISmM5",
        "outputId": "58cf813b-f41f-4fae-ad9b-e3bb67ac0640"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'A customer\\'s coffee machine arrived broken. \"This heartbreaking display of negligence shattered my dreams,\" writes the customer. \"I was emotionally distraught and inconsolable,\" he adds. \"It was like a war-torn soldier who had fought valiantly on the fields of some espressobattlefield\"'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Not so bad summary for a small model."
      ],
      "metadata": {
        "id": "K6nE7p2oTU1u"
      }
    }
  ]
}