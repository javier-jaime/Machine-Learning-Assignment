{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Train Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Random Forest classifier with Apache SparkML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark==2.4.8 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (2.4.8)\n",
      "Requirement already satisfied: wget==3.2 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (3.2)\n",
      "Requirement already satisfied: pyspark2pmml==0.5.1 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (0.5.1)\n",
      "Requirement already satisfied: py4j==0.10.7 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from pyspark==2.4.8) (0.10.7)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "export version=`python --version |awk '{print $2}' |awk -F\".\" '{print $1$2}'`\n",
    "\n",
    "if [ $version == '37' ]; then\n",
    "    pip install pyspark==2.4.8 wget==3.2 pyspark2pmml==0.5.1\n",
    "elif [ $version == '38' ]; then\n",
    "    pip install pyspark==3.1.2 wget==3.2 pyspark2pmml==0.5.1\n",
    "else\n",
    "    echo 'Currently only python 3.6 and 3.8 is supported, in case you need a different version please open an issue at https://github.com/elyra-ai/component-library/issues'\n",
    "    exit -1\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf, SQLContext\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark2pmml import PMMLBuilder\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "import logging\n",
    "import shutil\n",
    "import site\n",
    "import sys\n",
    "import wget\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sys.version[0:3] == '3.8':\n",
    "    url = ('https://github.com/jpmml/jpmml-sparkml/releases/download/1.7.2/'\n",
    "           'jpmml-sparkml-executable-1.7.2.jar')\n",
    "    wget.download(url)\n",
    "    shutil.copy('jpmml-sparkml-executable-1.7.2.jar',\n",
    "                site.getsitepackages()[0] + '/pyspark/jars/')\n",
    "elif sys.version[0:3] == '3.7':\n",
    "    url = ('https://github.com/jpmml/jpmml-sparkml/releases/download/1.5.12/'\n",
    "           'jpmml-sparkml-executable-1.5.12.jar')\n",
    "    wget.download(url)\n",
    "else:\n",
    "    raise Exception('Currently only python 3.6 and 3.8 is supported, in case '\n",
    "                    'you need a different version please open an issue at '\n",
    "                    'https://github.com/elyra-ai/component-library/issues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_parquet = os.environ.get('data_parquet',\n",
    "                              'data.parquet')  # input file name (parquet)\n",
    "master = os.environ.get('master',\n",
    "                        \"local[*]\")  # URL to Spark master\n",
    "model_target = os.environ.get('model_target',\n",
    "                              \"model.xml\")  # model output file name\n",
    "data_dir = os.environ.get('data_dir',\n",
    "                          '../../data/')  # temporary directory for data\n",
    "input_columns = os.environ.get('input_columns',\n",
    "                               '[\"x\", \"y\", \"z\"]')  # input columns to consider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = list(\n",
    "  map(\n",
    "      lambda s: re.sub('$', '\"', s),\n",
    "      map(\n",
    "          lambda s: s.replace('=', '=\"'),\n",
    "          filter(\n",
    "              lambda s: s.find('=') > -1 and bool(re.match('[A-Za-z0-9_]*=[.\\/A-Za-z0-9]*', s)),\n",
    "              sys.argv\n",
    "          )\n",
    "      )\n",
    "  )\n",
    ")\n",
    "\n",
    "for parameter in parameters:\n",
    "    logging.warning('Parameter: '+parameter) \n",
    "    exec(parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/10/12 21:26:47 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "21/10/12 21:26:49 WARN util.Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "conf = SparkConf().setMaster(master)\n",
    "if sys.version[0:3] == '3.7':\n",
    "    conf.set(\"spark.jars\", 'jpmml-sparkml-executable-1.5.12.jar')\n",
    "\n",
    "sc = SparkContext.getOrCreate(conf)\n",
    "sqlContext = SQLContext(sc)\n",
    "spark = sqlContext.sparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the parquet file and convert to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_csv = os.environ.get('data_csv', 'data.csv')\n",
    "data_parquet = os.environ.get('data_parquet', 'data.parquet')\n",
    "master = os.environ.get('master', \"local[*]\")\n",
    "data_dir = os.environ.get('data_dir', '../../data/')\n",
    "data_parquet = 'data.parquet'\n",
    "data_csv = 'parquet_to_csv.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip = False\n",
    "if os.path.exists(data_dir + data_csv):\n",
    "    skip = True\n",
    "if not skip:\n",
    "    if os.path.exists(data_dir + data_csv):\n",
    "        shutil.rmtree(data_dir + data_csv)\n",
    "    df.coalesce(1).write.option(\"header\", \"true\").csv(data_dir + data_csv)\n",
    "    file = glob.glob(data_dir + data_csv + '/part-*')\n",
    "    shutil.move(file[0], data_dir + data_csv + '.tmp')\n",
    "    shutil.rmtree(data_dir + data_csv)\n",
    "    shutil.move(data_dir + data_csv + '.tmp', data_dir + data_csv)\n",
    "    \n",
    "df_rf = spark.read.option(\"header\",True).csv(data_dir + \"parquet_to_csv.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- x: string (nullable = true)\n",
      " |-- y: string (nullable = true)\n",
      " |-- z: string (nullable = true)\n",
      " |-- source: string (nullable = true)\n",
      " |-- class: string (nullable = true)\n",
      "\n",
      "+---+---+---+--------------------+--------+\n",
      "|  x|  y|  z|              source|   class|\n",
      "+---+---+---+--------------------+--------+\n",
      "| 33| 36| 51|Accelerometer-201...|Eat_meat|\n",
      "| 33| 36| 51|Accelerometer-201...|Eat_meat|\n",
      "| 33| 35| 53|Accelerometer-201...|Eat_meat|\n",
      "| 31| 37| 52|Accelerometer-201...|Eat_meat|\n",
      "| 32| 36| 52|Accelerometer-201...|Eat_meat|\n",
      "| 32| 36| 51|Accelerometer-201...|Eat_meat|\n",
      "| 32| 36| 51|Accelerometer-201...|Eat_meat|\n",
      "| 33| 36| 53|Accelerometer-201...|Eat_meat|\n",
      "| 33| 35| 52|Accelerometer-201...|Eat_meat|\n",
      "| 33| 36| 52|Accelerometer-201...|Eat_meat|\n",
      "| 32| 35| 53|Accelerometer-201...|Eat_meat|\n",
      "| 33| 36| 52|Accelerometer-201...|Eat_meat|\n",
      "| 32| 38| 53|Accelerometer-201...|Eat_meat|\n",
      "| 32| 37| 52|Accelerometer-201...|Eat_meat|\n",
      "| 33| 35| 52|Accelerometer-201...|Eat_meat|\n",
      "| 32| 36| 53|Accelerometer-201...|Eat_meat|\n",
      "| 32| 36| 53|Accelerometer-201...|Eat_meat|\n",
      "| 32| 36| 52|Accelerometer-201...|Eat_meat|\n",
      "| 34| 36| 52|Accelerometer-201...|Eat_meat|\n",
      "| 33| 36| 52|Accelerometer-201...|Eat_meat|\n",
      "+---+---+---+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_rf = spark.read.option(\"header\",True).csv(data_dir + \"parquet_to_csv.csv\")\n",
    "df_rf.printSchema()\n",
    "df_rf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# register a corresponding query table\n",
    "df_rf.createOrReplaceTempView('df_rf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "df_rf = df_rf.withColumn(\"x\", df_rf.x.cast(DoubleType()))\n",
    "df_rf = df_rf.withColumn(\"y\", df_rf.y.cast(DoubleType()))\n",
    "df_rf = df_rf.withColumn(\"z\", df_rf.z.cast(DoubleType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits_rf = df_rf.randomSplit([0.8, 0.2], seed=1)\n",
    "df_train_rf = splits_rf[0]\n",
    "df_test_rf = splits_rf[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer = StringIndexer(inputCol=\"class\", outputCol=\"label\")\n",
    "\n",
    "vectorAssembler = VectorAssembler(inputCols=eval(input_columns),\n",
    "                                  outputCol=\"features\")\n",
    "\n",
    "normalizer = MinMaxScaler(inputCol=\"features\", outputCol=\"features_norm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 \t 5 \t 0.4397841007977302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 \t 7 \t 0.4631116234691051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 \t 5 \t 0.4431194021080671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 157:=======>                                                 (1 + 7) / 8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 \t 7 \t 0.46803063328753824\n",
      "Best parameters: Trees= 20 Depth= 7 Accuracy 0.46803063328753824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "maxAccuracy=0\n",
    "for trees in {10, 20}:\n",
    "    for depth in {5, 7}:\n",
    "        rf = RandomForestClassifier().setMaxDepth(depth).setNumTrees(trees).setSeed(1)\n",
    "        pipeline_rf = Pipeline(stages=[indexer, vectorAssembler, normalizer, rf])\n",
    "        model_rf = pipeline_rf.fit(df_train_rf)\n",
    "        prediction_rf = model_rf.transform(df_train_rf)\n",
    "        binEval_rf = MulticlassClassificationEvaluator(). \\\n",
    "            setMetricName(\"accuracy\"). \\\n",
    "            setPredictionCol(\"prediction\"). \\\n",
    "            setLabelCol(\"label\")\n",
    "        Accuracy = binEval_rf.evaluate(prediction_rf)\n",
    "        print(trees, \"\\t\", depth, \"\\t\", Accuracy)\n",
    "        if (maxAccuracy < Accuracy):\n",
    "            maxtrees = trees\n",
    "            maxdepth = depth\n",
    "            maxAccuracy = Accuracy\n",
    "\n",
    "print(\"Best parameters: Trees=\", maxtrees, \"Depth=\", maxdepth, \"Accuracy\", maxAccuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 55.042719,
   "end_time": "2021-01-28T16:00:26.871724",
   "environment_variables": {},
   "exception": null,
   "input_path": "/home/jovyan/work/elyra-classification/train-trusted-ai.ipynb",
   "output_path": "/home/jovyan/work/elyra-classification/train-trusted-ai.ipynb",
   "parameters": {},
   "start_time": "2021-01-28T15:59:31.829005",
   "version": "2.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
